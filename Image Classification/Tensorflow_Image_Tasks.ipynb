{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "iZxzdCYdFz_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Task"
      ],
      "metadata": {
        "id": "qtqUDcHC7SpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unziping data\n",
        "!unzip /content/Pokemon_dataset.zip"
      ],
      "metadata": {
        "id": "iCb1OhoUF1Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading getting image path list\n",
        "\n",
        "folder_path = r\"/content/Pokemon_dataset\"\n",
        "image_path_list = os.listdir(os.path.join(folder_path,'images'))\n",
        "print(image_path_list)"
      ],
      "metadata": {
        "id": "npGTolv0GNE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = r\"/content/Pokemon_dataset/pokemon.csv\"\n",
        "\n",
        "# Define the missing values in the \"Type2\" column\n",
        "na_values = {'Type2': ['']}\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "data = pd.read_csv(csv_file, na_values=na_values)\n",
        "\n",
        "# Print the DataFrame\n",
        "data = data.drop('Type2',axis=1)"
      ],
      "metadata": {
        "id": "iFwKkEiWF_NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding image path column to dataframe \n",
        "\n",
        "def image_path_creator(name):\n",
        "  name = [i for i in image_path_list if name in i.split('.')][0]\n",
        "  return os.path.join(folder_path,'images',name)\n",
        "\n",
        "data['Image_path'] = data['Name'].apply(image_path_creator)\n",
        "data"
      ],
      "metadata": {
        "id": "girodkFqIihj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding labels\n",
        "type_encode = preprocessing.LabelEncoder()\n",
        "data['type1_label'] = type_encode.fit_transform(data['Type1'].values.tolist())\n",
        "\n",
        "print('Number of classes = ',len(type_encode.classes_))"
      ],
      "metadata": {
        "id": "EXB158FFYxwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing images\n",
        "\n",
        "def preprocess_data(image_path,label):\n",
        "\n",
        "  # Reading image\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.io.decode_png(img,channels=3)\n",
        "\n",
        "  # Resizeing image\n",
        "  img = tf.image.resize(img,(120,120))\n",
        "\n",
        "  # Normalizing image \n",
        "  img = tf.image.convert_image_dtype(img,tf.float32)\n",
        "\n",
        "  return img,label"
      ],
      "metadata": {
        "id": "WsVxQwVqMqqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataset and AUTOTUNE for optimization\n",
        "\n",
        "image_list = data['Image_path'].values.tolist()\n",
        "label_list = data['type1_label'].values.tolist()\n",
        "\n",
        "\n",
        "# Perform train-test split\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "                    image_list, label_list, test_size=0.2, random_state=42)  # Adjust test_size as desired\n",
        "\n",
        "\n",
        "def configure_for_performance(dataset, batch_size):\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "# Create the train dataset\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "train_dataset = train_dataset.map(preprocess_data)\n",
        "\n",
        "# Pipeline of Prefetch and use AUTOTUNE for optimization\n",
        "train_dataset = configure_for_performance(train_dataset, TRAIN_BATCH_SIZE)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Create the train/val dataset\n",
        "TEST_BATCH_SIZE = 8\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
        "test_dataset = test_dataset.map(preprocess_data)\n",
        "\n",
        "# Pipeline of Prefetch and use AUTOTUNE for optimization\n",
        "test_dataset = test_dataset.shuffle(buffer_size=len(test_labels)).batch(TEST_BATCH_SIZE)\n",
        "# test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "5I14T1VoRpMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Sequential model\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(2, (5, 5),activation='relu', input_shape=(120, 120, 3),kernel_initializer='he_normal'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(4, (3, 3),strides=2 ,activation='relu',kernel_initializer='he_normal'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(8, (3, 3) ,activation='relu',kernel_initializer='he_normal'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu',kernel_initializer='he_normal'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(len(type_encode.classes_),activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TrKM5bWRUwA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf \"/content/logs\"\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "DOa04kBJXRZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate = 1e-4),\n",
        "              loss='sparse_categorical_crossentropy',  # Use 'categorical_crossentropy' if labels are one-hot encoded\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "TRAIN_STEPS_PER_EPOCH = len(train_paths) // TRAIN_BATCH_SIZE"
      ],
      "metadata": {
        "id": "nsO-Id3_UzN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting model\n",
        "model.fit(train_dataset,\n",
        "          steps_per_epoch= TRAIN_STEPS_PER_EPOCH,\n",
        "          validation_data = test_dataset,\n",
        "          # validation_steps = TEST_STEPS_PER_EPOCH,\n",
        "          epochs = 10, \n",
        "          callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "S6l6Yl_ac4a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "n8XuJUDGnRka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model"
      ],
      "metadata": {
        "id": "AG2Bzjei7L26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Predict output from test dataset\n",
        "test_predictions = model.predict(test_dataset)\n",
        "# Convert predictions to class labels\n",
        "test_predictions = tf.argmax(test_predictions, axis=1).numpy()\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "print('Loss:', loss)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "KePf12ZwyTlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute classification report\n",
        "report = classification_report(test_labels, test_predictions)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "wc65m2dh1feI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}